{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPuXE54YLlcq"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://jpbarddal.github.io/assets/data/bigdata/transactions_amostra.csv.zip\n",
        "!unzip transactions_amostra.csv.zip"
      ],
      "metadata": {
        "id": "BsZpY5JTL0hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Inicializa o Spark\n",
        "spark = SparkSession.builder\\\n",
        "    .master('local[*]')\\\n",
        "    .appName(\"\")\\\n",
        "    .getOrCreate()\n",
        "    \n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Carrega o conjunto de dados a partir do arquivo CSV\n",
        "dataset = sc.textFile(\"transactions_amostra.csv\")"
      ],
      "metadata": {
        "id": "x5JejyizL7d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printando as colunas\n",
        "\n",
        "columns = dataset.take(1)[0].split(\";\")\n",
        "print(\"Colunas do dataset:\")\n",
        "for column in columns:\n",
        "    print(column)\n"
      ],
      "metadata": {
        "id": "mmGbiZc9MsPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problema 1: O número de transações envolvendo o Brasil\n",
        "\n",
        "> Bloco com recuo\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cpjTf_JHNJFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"tde2\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "df = sc.textFile('transactions_amostra.csv')\n",
        "\n",
        "lines = df.map(lambda line: line.split(\";\"))\n",
        "\n",
        "brazil = lines.filter(lambda line: line[0] == 'Brazil')\n",
        "\n",
        "count = brazil.count()\n",
        "\n",
        "print(count)"
      ],
      "metadata": {
        "id": "66kXIbxeM3Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problema 2: O número de transações por tipo de fluxo e ano\n"
      ],
      "metadata": {
        "id": "SWRa096ONfiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapeia as transações para o par ((tipo de fluxo, ano), 1)\n",
        "transacoes_por_fluxo_ano = dataset.map(lambda linha: ((linha.split(\";\")[4], linha.split(\";\")[1]), 1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Itera sobre os resultados e imprime o tipo de fluxo, ano e contagem de transações\n",
        "for fluxo_ano, contagem in transacoes_por_fluxo_ano.collect():\n",
        "    print(\"Tipo de fluxo e ano:\", fluxo_ano, contagem)"
      ],
      "metadata": {
        "id": "_B2MxyOlNd1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problema 3: A média dos valores das commodities por ano\n",
        "\n"
      ],
      "metadata": {
        "id": "a1nNGzCfPad2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtra as linhas que possuem um valor numérico válido na coluna de valores de commodities\n",
        "valores_commodities = dataset.filter(lambda linha: linha.split(\";\")[5].replace(\".\", \"\").isdigit()) \\\n",
        "    .map(lambda linha: (linha.split(\";\")[1], float(linha.split(\";\")[5])))\n",
        "\n",
        "# Calcula a soma e contagem dos valores das commodities por ano usando a função aggregateByKey\n",
        "valores_commodities_por_ano = valores_commodities.aggregateByKey((0.0, 0),\n",
        "                                                                  lambda acumulador, valor: (acumulador[0] + valor, acumulador[1] + 1),\n",
        "                                                                  lambda acumulador1, acumulador2: (acumulador1[0] + acumulador2[0], acumulador1[1] + acumulador2[1]))\n",
        "\n",
        "# Calcula a média dos valores das commodities por ano usando a função mapValues\n",
        "media_valores_por_ano = valores_commodities_por_ano.mapValues(lambda acumulador: acumulador[0] / acumulador[1])\n",
        "\n",
        "# Itera sobre os resultados e imprime o valor médio da commodity para cada ano\n",
        "for ano, media in media_valores_por_ano.collect():\n",
        "    print(\"Valor médio da commodity para o ano\", ano, \":\", media)"
      ],
      "metadata": {
        "id": "t2QADGCzNmaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problema 4: O preço médio das commodities por tipo de unidade, ano e categoria no fluxo de exportação no Brasil\n"
      ],
      "metadata": {
        "id": "p5xvFyvnP85l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = sc.textFile('transactions_amostra.csv')\n",
        "\n",
        "pairrdd = df.filter(lambda x: \"Brazil\" in x and x.split(';')[4] == \"Export\")\n",
        "pairrdd = pairrdd.map(lambda x: ((x.split(';')[1], x.split(';')[7], x.split(';')[3], x.split(';')[8]), float(x.split(';')[5])))\n",
        "\n",
        "soma = pairrdd.aggregateByKey((0.0, 0), lambda x,y: (x[0] + y, x[1] + 1), lambda x,y: (x[0] + y[0], x[1] + y[1]))\n",
        "media = soma.mapValues(lambda x: x[0]/x[1])\n",
        "\n",
        "media.take(5)"
      ],
      "metadata": {
        "id": "FBBPTvPKPLqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problema 5: O preço máximo, mínimo e médio por tipo de unidade e ano\n"
      ],
      "metadata": {
        "id": "2CXlj7zsQjs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Processar o conjunto de dados\n",
        "transacoes = dataset.map(lambda linha: ((linha.split(\";\")[7], linha.split(\";\")[1]), float(linha.split(\";\")[5].replace(\",\", \".\")) if linha.split(\";\")[5].replace(\",\", \".\").replace(\".\", \"\").isdigit() else 0))\n",
        "\n",
        "# Filtrar transações com preços inválidos\n",
        "transacoes_validas = transacoes.filter(lambda transacao: transacao[1] != 0)\n",
        "\n",
        "# Calcular preço máximo, mínimo e médio por tipo de unidade e ano\n",
        "estatisticas_preco_por_unidade_ano = transacoes_validas.aggregateByKey(\n",
        "    (float('-inf'), float('inf'), 0.0, 0),\n",
        "    lambda a, b: (max(a[0], b), min(a[1], b), a[2] + b, a[3] + 1),\n",
        "    lambda a, b: (max(a[0], b[0]), min(a[1], b[1]), a[2] + b[2], a[3] + b[3])\n",
        ")\n",
        "\n",
        "# Calcular preço médio da transação\n",
        "preco_medio_por_unidade_ano = estatisticas_preco_por_unidade_ano.mapValues(lambda v: (v[0], v[1], v[2] / v[3]))\n",
        "\n",
        "# Iterar sobre os resultados e imprimir preço máximo, mínimo e médio por tipo de unidade e ano\n",
        "for (unidade_ano, (preco_maximo, preco_minimo, preco_medio)) in preco_medio_por_unidade_ano.collect():\n",
        "    tipo_unidade, ano = unidade_ano\n",
        "    print(\"Tipo de Unidade:\", tipo_unidade)\n",
        "    print(\"Ano:\", ano)\n",
        "    print(\"Preço Máximo:\", preco_maximo)\n",
        "    print(\"Preço Mínimo:\", preco_minimo)\n",
        "    print(\"Preço Médio:\", preco_medio)\n",
        "    print()"
      ],
      "metadata": {
        "id": "HxVBpN3gYjv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problema 6: O país com o maior preço médio de commodities na categoria Exportação\n"
      ],
      "metadata": {
        "id": "iuWZeMLiVoNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtra o conjunto de dados para a categoria de exportação\n",
        "exportacao = dataset.filter(lambda line: line.split(\";\")[4] == \"Export\")\n",
        "\n",
        "# Mapeia cada linha para pares chave-valor (país, preço)\n",
        "preco_por_pais = exportacao.map(lambda line: (line.split(\";\")[0], float(line.split(\";\")[5])))\n",
        "\n",
        "# Calcula a soma e contagem dos preços para cada país\n",
        "soma_contagem_por_pais = preco_por_pais.aggregateByKey(\n",
        "    (0.0, 0),\n",
        "    lambda acc, price: (acc[0] + price, acc[1] + 1),\n",
        "    lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
        ")\n",
        "\n",
        "# Calcula o preço médio para cada país\n",
        "preco_medio_por_pais = soma_contagem_por_pais.mapValues(lambda acc: acc[0] / acc[1])\n",
        "\n",
        "# Encontra o país com o maior preço médio\n",
        "pais_com_maior_preco_medio = preco_medio_por_pais.max(lambda x: x[1])\n",
        "\n",
        "# Imprime o resultado\n",
        "print(\"País com o maior preço médio de commodities na categoria Exportação:\",\n",
        "      pais_com_maior_preco_medio[0], pais_com_maior_preco_medio[1])"
      ],
      "metadata": {
        "id": "jGLwcup3VqNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problema 7: A commodity mais comercializada (somando as quantidades) em 2016 por tipo de fluxo\n"
      ],
      "metadata": {
        "id": "TbH4eQa0WB4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"tde2\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "df = sc.textFile('transactions_amostra.csv')\n",
        "\n",
        "pairrdd = df.filter(lambda x: x.split(';')[1] == \"2016\")\n",
        "\n",
        "pairrdd = pairrdd.map(lambda x: (x.split(';')[3], (x.split(';')[4], float(x.split(';')[8]))))\n",
        "\n",
        "export = pairrdd.filter(lambda x: x[1][0] == \"Export\")\n",
        "export = export.map(lambda x: (x[0], x[1][1]))\n",
        "\n",
        "somaExport = export.reduceByKey(lambda x,y: x+y)\n",
        "maiorExport = somaExport.max(lambda x: x[1])\n",
        "\n",
        "importa = pairrdd.filter(lambda x: x[1][0] == \"Import\")\n",
        "importa = importa.map(lambda x: (x[0], x[1][1]))\n",
        "\n",
        "somaImporta = importa.reduceByKey(lambda x,y: x+y)\n",
        "maiorImporta = somaImporta.max(lambda x: x[1])\n",
        "\n",
        "ReExport = pairrdd.filter(lambda x: x[1][0] == \"Re-Export\")\n",
        "ReExport = ReExport.map(lambda x: (x[0], x[1][1]))\n",
        "\n",
        "somaReExport = ReExport.reduceByKey(lambda x,y: x+y)\n",
        "maiorReExport = somaReExport.max(lambda x: x[1])\n",
        "\n",
        "ReImport = pairrdd.filter(lambda x: x[1][0] == \"Re-Import\")\n",
        "ReImport = ReImport.map(lambda x: (x[0], x[1][1]))\n",
        "\n",
        "somaReImport = ReImport.reduceByKey(lambda x,y: x+y)\n",
        "maiorReImport = somaReImport.max(lambda x: x[1])\n",
        "\n",
        "print(\"Export\")\n",
        "print(maiorExport)\n",
        "print(\"Import\")\n",
        "print(maiorImporta)\n",
        "print(\"Re-Export\")\n",
        "print(maiorReExport)\n",
        "print(\"Re-Import\")\n",
        "print(maiorReImport)"
      ],
      "metadata": {
        "id": "lPs4_fFXWAr2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}